{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VwtIuPVsjRpw",
        "outputId": "4ed62b95-bd86-4855-9f5d-e3da2e2129d3"
      },
      "id": "VwtIuPVsjRpw",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "27dd3312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "27dd3312",
        "outputId": "f7f39c02-f2d5-4763-a2af-5972c933c908"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f83bf752ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701c7522",
      "metadata": {
        "id": "701c7522"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "be5a4cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "be5a4cfb",
        "outputId": "6e8afc7a-0ab0-4795-cb57-ce08e6ae8bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "data_path = '/content/drive/MyDrive/ML'\n",
        "cifar10_Train = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_Validate = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f0ed321b",
      "metadata": {
        "id": "f0ed321b"
      },
      "outputs": [],
      "source": [
        "#cofar10 train loader\n",
        "train_loader = torch.utils.data.DataLoader(cifar10_Train, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_Validate, batch_size=64,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e5813e8f",
      "metadata": {
        "id": "e5813e8f"
      },
      "outputs": [],
      "source": [
        "#1a - ANN with one hidden layer\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38958a6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38958a6a",
        "outputId": "91bc2fe5-0da1-41fe-8105-db84c3225144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 2.021203\n",
            "Epoch 1, Loss 1.709806\n",
            "Epoch 2, Loss 1.709243\n",
            "Epoch 3, Loss 1.926439\n",
            "Epoch 4, Loss 1.552730\n",
            "Epoch 5, Loss 1.999883\n",
            "Epoch 6, Loss 1.733852\n",
            "Epoch 7, Loss 1.806202\n",
            "Epoch 8, Loss 1.790688\n",
            "Epoch 9, Loss 1.734670\n",
            "Epoch 10, Loss 1.683239\n",
            "Epoch 11, Loss 2.074657\n",
            "Epoch 12, Loss 1.775076\n",
            "Epoch 13, Loss 1.985622\n",
            "Epoch 14, Loss 1.683341\n",
            "Epoch 15, Loss 1.747572\n",
            "Epoch 16, Loss 1.824901\n",
            "Epoch 17, Loss 1.427613\n",
            "Epoch 18, Loss 1.620868\n",
            "Epoch 19, Loss 1.770268\n",
            "Epoch 20, Loss 1.300837\n",
            "Epoch 21, Loss 1.628060\n",
            "Epoch 22, Loss 1.578346\n",
            "Epoch 23, Loss 1.677974\n",
            "Epoch 24, Loss 1.260445\n",
            "Epoch 25, Loss 1.391607\n",
            "Epoch 26, Loss 1.635625\n",
            "Epoch 27, Loss 1.606100\n",
            "Epoch 28, Loss 1.507445\n",
            "Epoch 29, Loss 1.012500\n",
            "Epoch 30, Loss 1.367685\n",
            "Epoch 31, Loss 1.513155\n",
            "Epoch 32, Loss 1.444149\n",
            "Epoch 33, Loss 1.839713\n",
            "Epoch 34, Loss 1.220399\n",
            "Epoch 35, Loss 1.622929\n",
            "Epoch 36, Loss 1.769524\n",
            "Epoch 37, Loss 1.732202\n",
            "Epoch 38, Loss 1.605392\n",
            "Epoch 39, Loss 1.348325\n",
            "Epoch 40, Loss 1.561189\n",
            "Epoch 41, Loss 1.622194\n",
            "Epoch 42, Loss 1.707099\n",
            "Epoch 43, Loss 1.476650\n",
            "Epoch 44, Loss 1.473745\n",
            "Epoch 45, Loss 1.939474\n",
            "Epoch 46, Loss 1.795142\n",
            "Epoch 47, Loss 1.644985\n",
            "Epoch 48, Loss 1.776452\n",
            "Epoch 49, Loss 1.449897\n",
            "Epoch 50, Loss 1.032484\n",
            "Epoch 51, Loss 1.692459\n",
            "Epoch 52, Loss 1.476396\n",
            "Epoch 53, Loss 1.213685\n",
            "Epoch 54, Loss 1.347739\n",
            "Epoch 55, Loss 1.210346\n",
            "Epoch 56, Loss 1.252887\n",
            "Epoch 57, Loss 1.425598\n",
            "Epoch 58, Loss 1.405917\n",
            "Epoch 59, Loss 1.602604\n",
            "Epoch 60, Loss 1.513097\n",
            "Epoch 61, Loss 1.326229\n",
            "Epoch 62, Loss 1.373105\n",
            "Epoch 63, Loss 1.697931\n",
            "Epoch 64, Loss 1.311838\n",
            "Epoch 65, Loss 1.855392\n",
            "Epoch 66, Loss 1.205334\n",
            "Epoch 67, Loss 1.362313\n",
            "Epoch 68, Loss 1.381053\n",
            "Epoch 69, Loss 1.916706\n",
            "Epoch 70, Loss 1.501180\n",
            "Epoch 71, Loss 1.370865\n",
            "Epoch 72, Loss 1.553648\n",
            "Epoch 73, Loss 1.476770\n",
            "Epoch 74, Loss 1.320859\n",
            "Epoch 75, Loss 1.391861\n",
            "Epoch 76, Loss 1.172233\n",
            "Epoch 77, Loss 1.280458\n",
            "Epoch 78, Loss 2.299248\n",
            "Epoch 79, Loss 1.031428\n",
            "Epoch 80, Loss 1.329990\n",
            "Epoch 81, Loss 1.293442\n",
            "Epoch 82, Loss 1.368505\n",
            "Epoch 83, Loss 1.122461\n",
            "Epoch 84, Loss 1.124512\n",
            "Epoch 85, Loss 1.490213\n",
            "Epoch 86, Loss 1.818675\n",
            "Epoch 87, Loss 1.114458\n",
            "Epoch 88, Loss 1.018894\n",
            "Epoch 89, Loss 1.635675\n",
            "Epoch 90, Loss 1.309022\n",
            "Epoch 91, Loss 1.326979\n",
            "Epoch 92, Loss 1.607821\n",
            "Epoch 93, Loss 1.085565\n",
            "Epoch 94, Loss 1.154342\n",
            "Epoch 95, Loss 0.998148\n",
            "Epoch 96, Loss 1.248807\n",
            "Epoch 97, Loss 1.425686\n",
            "Epoch 98, Loss 1.312167\n",
            "Epoch 99, Loss 0.740580\n",
            "Epoch 100, Loss 1.134153\n",
            "Epoch 101, Loss 1.658089\n",
            "Epoch 102, Loss 1.234080\n",
            "Epoch 103, Loss 1.302969\n",
            "Epoch 104, Loss 1.072528\n",
            "Epoch 105, Loss 1.239783\n",
            "Epoch 106, Loss 1.098390\n",
            "Epoch 107, Loss 1.298348\n",
            "Epoch 108, Loss 1.061735\n",
            "Epoch 109, Loss 1.701712\n",
            "Epoch 110, Loss 1.009118\n",
            "Epoch 111, Loss 1.028158\n",
            "Epoch 112, Loss 0.981338\n",
            "Epoch 113, Loss 1.480038\n",
            "Epoch 114, Loss 1.307224\n",
            "Epoch 115, Loss 0.745043\n",
            "Epoch 116, Loss 1.095491\n",
            "Epoch 117, Loss 1.160128\n",
            "Epoch 118, Loss 1.073159\n",
            "Epoch 119, Loss 1.425418\n",
            "Epoch 120, Loss 0.933866\n",
            "Epoch 121, Loss 0.836047\n",
            "Epoch 122, Loss 1.056166\n",
            "Epoch 123, Loss 1.890583\n",
            "Epoch 124, Loss 1.066707\n",
            "Epoch 125, Loss 0.751878\n",
            "Epoch 126, Loss 0.892016\n",
            "Epoch 127, Loss 0.930936\n",
            "Epoch 128, Loss 1.447676\n",
            "Epoch 129, Loss 1.261523\n",
            "Epoch 130, Loss 1.346910\n",
            "Epoch 131, Loss 0.897215\n",
            "Epoch 132, Loss 1.026524\n",
            "Epoch 133, Loss 1.191852\n",
            "Epoch 134, Loss 0.990029\n",
            "Epoch 135, Loss 0.997427\n",
            "Epoch 136, Loss 1.008341\n",
            "Epoch 137, Loss 0.830874\n",
            "Epoch 138, Loss 1.332801\n",
            "Epoch 139, Loss 1.023569\n",
            "Epoch 140, Loss 1.294956\n",
            "Epoch 141, Loss 1.099586\n",
            "Epoch 142, Loss 0.760491\n",
            "Epoch 143, Loss 0.803725\n",
            "Epoch 144, Loss 1.067458\n",
            "Epoch 145, Loss 1.119042\n",
            "Epoch 146, Loss 0.846435\n",
            "Epoch 147, Loss 1.016588\n",
            "Epoch 148, Loss 1.004027\n",
            "Epoch 149, Loss 0.972171\n",
            "Epoch 150, Loss 1.029840\n",
            "Epoch 151, Loss 0.979500\n",
            "Epoch 152, Loss 0.955917\n",
            "Epoch 153, Loss 0.909045\n",
            "Epoch 154, Loss 1.295163\n",
            "Epoch 155, Loss 1.341084\n",
            "Epoch 156, Loss 0.972553\n",
            "Epoch 157, Loss 1.014590\n",
            "Epoch 158, Loss 0.937018\n",
            "Epoch 159, Loss 1.496661\n",
            "Epoch 160, Loss 0.890984\n",
            "Epoch 161, Loss 0.953203\n",
            "Epoch 162, Loss 1.094615\n",
            "Epoch 163, Loss 1.301707\n",
            "Epoch 164, Loss 0.934956\n",
            "Epoch 165, Loss 0.762389\n",
            "Epoch 166, Loss 1.018083\n",
            "Epoch 167, Loss 1.120066\n",
            "Epoch 168, Loss 0.893857\n",
            "Epoch 169, Loss 1.240411\n",
            "Epoch 170, Loss 1.377643\n",
            "Epoch 171, Loss 0.984697\n",
            "Epoch 172, Loss 0.866854\n",
            "Epoch 173, Loss 1.041797\n",
            "Epoch 174, Loss 1.622164\n",
            "Epoch 175, Loss 0.887184\n",
            "Epoch 176, Loss 0.926033\n",
            "Epoch 177, Loss 0.989807\n",
            "Epoch 178, Loss 0.836471\n",
            "Epoch 179, Loss 0.981364\n",
            "Epoch 180, Loss 1.124432\n",
            "Epoch 181, Loss 0.983979\n",
            "Epoch 182, Loss 0.980900\n",
            "Epoch 183, Loss 1.223408\n",
            "Epoch 184, Loss 1.066851\n",
            "Epoch 185, Loss 1.109800\n",
            "Epoch 186, Loss 0.919599\n",
            "Epoch 187, Loss 0.944558\n",
            "Epoch 188, Loss 0.926117\n",
            "Epoch 189, Loss 1.223487\n",
            "Epoch 190, Loss 1.011961\n",
            "Epoch 191, Loss 1.091439\n",
            "Epoch 192, Loss 1.137280\n",
            "Epoch 193, Loss 1.163758\n",
            "Epoch 194, Loss 0.836288\n",
            "Epoch 195, Loss 0.837457\n",
            "Epoch 196, Loss 1.087309\n",
            "Epoch 197, Loss 0.392499\n",
            "Epoch 198, Loss 0.940682\n",
            "Epoch 199, Loss 0.669518\n",
            "Epoch 200, Loss 0.979801\n",
            "Epoch 201, Loss 1.162130\n",
            "Epoch 202, Loss 0.820263\n",
            "Epoch 203, Loss 0.943395\n",
            "Epoch 204, Loss 0.481647\n",
            "Epoch 205, Loss 1.124208\n",
            "Epoch 206, Loss 0.712831\n",
            "Epoch 207, Loss 0.833116\n",
            "Epoch 208, Loss 0.907911\n",
            "Epoch 209, Loss 0.853191\n",
            "Epoch 210, Loss 0.785390\n",
            "Epoch 211, Loss 0.780900\n",
            "Epoch 212, Loss 0.702413\n",
            "Epoch 213, Loss 0.822504\n",
            "Epoch 214, Loss 0.644999\n",
            "Epoch 215, Loss 1.012549\n",
            "Epoch 216, Loss 1.344969\n",
            "Epoch 217, Loss 0.441282\n",
            "Epoch 218, Loss 0.788454\n",
            "Epoch 219, Loss 0.648984\n",
            "Epoch 220, Loss 0.893303\n",
            "Epoch 221, Loss 0.402423\n",
            "Epoch 222, Loss 0.325390\n",
            "Epoch 223, Loss 0.972475\n",
            "Epoch 224, Loss 0.629187\n",
            "Epoch 225, Loss 0.668418\n",
            "Epoch 226, Loss 0.444101\n",
            "Epoch 227, Loss 0.722072\n",
            "Epoch 228, Loss 1.156533\n",
            "Epoch 229, Loss 0.861587\n",
            "Epoch 230, Loss 0.841691\n",
            "Epoch 231, Loss 1.016720\n",
            "Epoch 232, Loss 0.500845\n",
            "Epoch 233, Loss 0.651582\n",
            "Epoch 234, Loss 0.505145\n",
            "Epoch 235, Loss 0.292157\n",
            "Epoch 236, Loss 0.575610\n",
            "Epoch 237, Loss 0.407274\n",
            "Epoch 238, Loss 1.048246\n",
            "Epoch 239, Loss 0.484775\n",
            "Epoch 240, Loss 1.215928\n",
            "Epoch 241, Loss 0.735142\n",
            "Epoch 242, Loss 0.416182\n",
            "Epoch 243, Loss 0.714202\n",
            "Epoch 244, Loss 0.498981\n",
            "Epoch 245, Loss 0.683288\n",
            "Epoch 246, Loss 0.658367\n",
            "Epoch 247, Loss 0.577682\n",
            "Epoch 248, Loss 0.704742\n",
            "Epoch 249, Loss 0.613418\n",
            "Epoch 250, Loss 0.300785\n",
            "Epoch 251, Loss 0.721434\n",
            "Epoch 252, Loss 0.491821\n",
            "Epoch 253, Loss 0.991671\n",
            "Epoch 254, Loss 0.985459\n",
            "Epoch 255, Loss 0.433052\n",
            "Epoch 256, Loss 0.692436\n",
            "Epoch 257, Loss 0.288088\n",
            "Epoch 258, Loss 0.729527\n",
            "Epoch 259, Loss 0.844177\n",
            "Epoch 260, Loss 0.782480\n",
            "Epoch 261, Loss 0.395469\n",
            "Epoch 262, Loss 0.237384\n",
            "Epoch 263, Loss 0.600982\n",
            "Epoch 264, Loss 0.991411\n",
            "Epoch 265, Loss 0.683205\n",
            "Epoch 266, Loss 0.431696\n",
            "Epoch 267, Loss 0.446629\n",
            "Epoch 268, Loss 0.742862\n",
            "Epoch 269, Loss 0.632801\n",
            "Epoch 270, Loss 0.748431\n",
            "Epoch 271, Loss 0.705650\n",
            "Epoch 272, Loss 0.368489\n",
            "Epoch 273, Loss 0.833677\n",
            "Epoch 274, Loss 0.502279\n",
            "Epoch 275, Loss 0.412307\n",
            "Epoch 276, Loss 0.603248\n",
            "Epoch 277, Loss 0.677964\n",
            "Epoch 278, Loss 0.639389\n",
            "Epoch 279, Loss 1.129799\n",
            "Epoch 280, Loss 0.546305\n",
            "Epoch 281, Loss 0.448027\n",
            "Epoch 282, Loss 0.774810\n",
            "Epoch 283, Loss 0.310574\n",
            "Epoch 284, Loss 0.522477\n",
            "Epoch 285, Loss 0.707928\n",
            "Epoch 286, Loss 0.451973\n",
            "Epoch 287, Loss 0.724503\n",
            "Epoch 288, Loss 0.348427\n",
            "Epoch 289, Loss 0.580410\n",
            "Epoch 290, Loss 0.467765\n",
            "Epoch 291, Loss 0.620720\n",
            "Epoch 292, Loss 0.489618\n",
            "Epoch 293, Loss 0.945252\n",
            "Epoch 294, Loss 0.467329\n",
            "Epoch 295, Loss 1.004104\n",
            "Epoch 296, Loss 0.410457\n",
            "Epoch 297, Loss 0.369573\n",
            "Epoch 298, Loss 0.432527\n",
            "Epoch 299, Loss 0.440857\n"
          ]
        }
      ],
      "source": [
        "#1a - ANN - 1 hidden layer \n",
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print('Epoch %d, Loss %f' % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy for 1a - ANN - 1 hidden layer\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k0_d3L4kUa1",
        "outputId": "20500997-abfb-401f-e8b9-1de1e453f953"
      },
      "id": "2k0_d3L4kUa1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.491200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b3355f",
      "metadata": {
        "id": "e8b3355f"
      },
      "outputs": [],
      "source": [
        "#1b - ANN - 3 hidden layers model\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee58102",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aee58102",
        "outputId": "8084f438-3539-4dd1-cdb6-c3cd4f05af64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 1.893748\n",
            "Epoch 1, Loss 2.107429\n",
            "Epoch 2, Loss 1.667065\n",
            "Epoch 3, Loss 1.936298\n",
            "Epoch 4, Loss 1.727779\n",
            "Epoch 5, Loss 1.765209\n",
            "Epoch 6, Loss 1.567509\n",
            "Epoch 7, Loss 2.121211\n",
            "Epoch 8, Loss 2.192231\n",
            "Epoch 9, Loss 1.749575\n",
            "Epoch 10, Loss 1.483528\n",
            "Epoch 11, Loss 1.773197\n",
            "Epoch 12, Loss 1.841086\n",
            "Epoch 13, Loss 1.595950\n",
            "Epoch 14, Loss 1.598628\n",
            "Epoch 15, Loss 1.890113\n",
            "Epoch 16, Loss 1.795640\n",
            "Epoch 17, Loss 1.684035\n",
            "Epoch 18, Loss 1.830145\n",
            "Epoch 19, Loss 1.624609\n",
            "Epoch 20, Loss 1.317592\n",
            "Epoch 21, Loss 1.681274\n",
            "Epoch 22, Loss 1.464893\n",
            "Epoch 23, Loss 1.306412\n",
            "Epoch 24, Loss 1.583615\n",
            "Epoch 25, Loss 1.882489\n",
            "Epoch 26, Loss 2.087799\n",
            "Epoch 27, Loss 1.874099\n",
            "Epoch 28, Loss 1.526567\n",
            "Epoch 29, Loss 1.854965\n",
            "Epoch 30, Loss 1.348746\n",
            "Epoch 31, Loss 1.683541\n",
            "Epoch 32, Loss 1.530746\n",
            "Epoch 33, Loss 1.725124\n",
            "Epoch 34, Loss 2.067548\n",
            "Epoch 35, Loss 1.672197\n",
            "Epoch 36, Loss 1.648602\n",
            "Epoch 37, Loss 1.729274\n",
            "Epoch 38, Loss 1.601536\n",
            "Epoch 39, Loss 1.424357\n",
            "Epoch 40, Loss 1.736381\n",
            "Epoch 41, Loss 1.349189\n",
            "Epoch 42, Loss 1.619587\n",
            "Epoch 43, Loss 1.662500\n",
            "Epoch 44, Loss 1.348251\n",
            "Epoch 45, Loss 1.874473\n",
            "Epoch 46, Loss 1.556870\n",
            "Epoch 47, Loss 1.752677\n",
            "Epoch 48, Loss 1.456747\n",
            "Epoch 49, Loss 1.358333\n",
            "Epoch 50, Loss 1.359602\n",
            "Epoch 51, Loss 1.163184\n",
            "Epoch 52, Loss 1.624544\n",
            "Epoch 53, Loss 1.545835\n",
            "Epoch 54, Loss 1.854571\n",
            "Epoch 55, Loss 1.481753\n",
            "Epoch 56, Loss 1.628265\n",
            "Epoch 57, Loss 1.153714\n",
            "Epoch 58, Loss 1.604751\n",
            "Epoch 59, Loss 1.715634\n",
            "Epoch 60, Loss 2.136177\n",
            "Epoch 61, Loss 1.630529\n",
            "Epoch 62, Loss 1.246648\n",
            "Epoch 63, Loss 1.720971\n",
            "Epoch 64, Loss 1.788244\n",
            "Epoch 65, Loss 1.535894\n",
            "Epoch 66, Loss 1.183071\n",
            "Epoch 67, Loss 1.819583\n",
            "Epoch 68, Loss 1.527238\n",
            "Epoch 69, Loss 1.165366\n",
            "Epoch 70, Loss 1.303420\n",
            "Epoch 71, Loss 1.584063\n",
            "Epoch 72, Loss 1.368373\n",
            "Epoch 73, Loss 1.415142\n",
            "Epoch 74, Loss 1.311227\n",
            "Epoch 75, Loss 0.860111\n",
            "Epoch 76, Loss 1.126048\n",
            "Epoch 77, Loss 1.190723\n",
            "Epoch 78, Loss 1.446474\n",
            "Epoch 79, Loss 1.131913\n",
            "Epoch 80, Loss 1.365443\n",
            "Epoch 81, Loss 0.840487\n",
            "Epoch 82, Loss 1.207677\n",
            "Epoch 83, Loss 0.871555\n",
            "Epoch 84, Loss 1.197996\n",
            "Epoch 85, Loss 1.633026\n",
            "Epoch 86, Loss 1.226655\n",
            "Epoch 87, Loss 1.915197\n",
            "Epoch 88, Loss 1.177787\n",
            "Epoch 89, Loss 1.181653\n",
            "Epoch 90, Loss 1.598515\n",
            "Epoch 91, Loss 1.414803\n",
            "Epoch 92, Loss 1.448207\n",
            "Epoch 93, Loss 1.306828\n",
            "Epoch 94, Loss 1.179947\n",
            "Epoch 95, Loss 0.998869\n",
            "Epoch 96, Loss 1.109920\n",
            "Epoch 97, Loss 0.799539\n",
            "Epoch 98, Loss 1.051960\n",
            "Epoch 99, Loss 1.091185\n",
            "Epoch 100, Loss 0.765875\n",
            "Epoch 101, Loss 1.557746\n",
            "Epoch 102, Loss 1.102580\n",
            "Epoch 103, Loss 0.961525\n",
            "Epoch 104, Loss 1.220895\n",
            "Epoch 105, Loss 1.370752\n",
            "Epoch 106, Loss 0.836609\n",
            "Epoch 107, Loss 0.903284\n",
            "Epoch 108, Loss 0.701779\n",
            "Epoch 109, Loss 1.074680\n",
            "Epoch 110, Loss 0.765235\n",
            "Epoch 111, Loss 1.567846\n",
            "Epoch 112, Loss 1.345029\n",
            "Epoch 113, Loss 1.132392\n",
            "Epoch 114, Loss 0.928008\n",
            "Epoch 115, Loss 0.949983\n",
            "Epoch 116, Loss 0.637474\n",
            "Epoch 117, Loss 0.739055\n",
            "Epoch 118, Loss 0.736762\n",
            "Epoch 119, Loss 0.547953\n",
            "Epoch 120, Loss 0.947574\n",
            "Epoch 121, Loss 1.047684\n",
            "Epoch 122, Loss 0.898773\n",
            "Epoch 123, Loss 0.783045\n",
            "Epoch 124, Loss 0.799663\n",
            "Epoch 125, Loss 0.539797\n",
            "Epoch 126, Loss 1.168353\n",
            "Epoch 127, Loss 0.856663\n",
            "Epoch 128, Loss 0.931975\n",
            "Epoch 129, Loss 0.583404\n",
            "Epoch 130, Loss 1.390772\n",
            "Epoch 131, Loss 0.791287\n",
            "Epoch 132, Loss 0.803911\n",
            "Epoch 133, Loss 0.543117\n",
            "Epoch 134, Loss 1.047063\n",
            "Epoch 135, Loss 0.452426\n",
            "Epoch 136, Loss 0.717172\n",
            "Epoch 137, Loss 0.681433\n",
            "Epoch 138, Loss 0.766564\n",
            "Epoch 139, Loss 0.846431\n",
            "Epoch 140, Loss 0.677331\n",
            "Epoch 141, Loss 0.422608\n",
            "Epoch 142, Loss 0.860876\n",
            "Epoch 143, Loss 0.427766\n",
            "Epoch 144, Loss 0.649340\n",
            "Epoch 145, Loss 0.844551\n",
            "Epoch 146, Loss 0.419098\n",
            "Epoch 147, Loss 0.744781\n",
            "Epoch 148, Loss 0.750761\n",
            "Epoch 149, Loss 0.617682\n",
            "Epoch 150, Loss 0.693242\n",
            "Epoch 151, Loss 0.301296\n",
            "Epoch 152, Loss 0.910581\n",
            "Epoch 153, Loss 0.904075\n",
            "Epoch 154, Loss 0.586224\n",
            "Epoch 155, Loss 0.400268\n",
            "Epoch 156, Loss 0.477962\n",
            "Epoch 157, Loss 0.281657\n",
            "Epoch 158, Loss 0.924726\n",
            "Epoch 159, Loss 0.511542\n",
            "Epoch 160, Loss 0.500729\n",
            "Epoch 161, Loss 0.185665\n",
            "Epoch 162, Loss 0.597242\n",
            "Epoch 163, Loss 0.536204\n",
            "Epoch 164, Loss 0.565449\n",
            "Epoch 165, Loss 0.369053\n",
            "Epoch 166, Loss 0.455336\n",
            "Epoch 167, Loss 0.491963\n",
            "Epoch 168, Loss 0.322949\n",
            "Epoch 169, Loss 0.321798\n",
            "Epoch 170, Loss 0.443131\n",
            "Epoch 171, Loss 0.198088\n",
            "Epoch 172, Loss 0.240782\n",
            "Epoch 173, Loss 0.337313\n",
            "Epoch 174, Loss 0.344858\n",
            "Epoch 175, Loss 0.293903\n",
            "Epoch 176, Loss 0.337702\n",
            "Epoch 177, Loss 0.248200\n",
            "Epoch 178, Loss 0.284160\n",
            "Epoch 179, Loss 0.459730\n",
            "Epoch 180, Loss 0.304448\n",
            "Epoch 181, Loss 0.338291\n",
            "Epoch 182, Loss 0.168392\n",
            "Epoch 183, Loss 0.321377\n",
            "Epoch 184, Loss 0.124542\n",
            "Epoch 185, Loss 0.216117\n",
            "Epoch 186, Loss 0.503975\n",
            "Epoch 187, Loss 0.270376\n",
            "Epoch 188, Loss 0.224225\n",
            "Epoch 189, Loss 0.222001\n",
            "Epoch 190, Loss 0.241014\n",
            "Epoch 191, Loss 0.132066\n",
            "Epoch 192, Loss 0.437319\n",
            "Epoch 193, Loss 0.207507\n",
            "Epoch 194, Loss 0.260623\n",
            "Epoch 195, Loss 0.656579\n",
            "Epoch 196, Loss 0.114363\n",
            "Epoch 197, Loss 0.106798\n",
            "Epoch 198, Loss 0.114902\n",
            "Epoch 199, Loss 0.479839\n",
            "Epoch 200, Loss 0.117855\n",
            "Epoch 201, Loss 0.218863\n",
            "Epoch 202, Loss 0.233258\n",
            "Epoch 203, Loss 0.221393\n",
            "Epoch 204, Loss 0.124858\n",
            "Epoch 205, Loss 0.091310\n",
            "Epoch 206, Loss 0.117510\n",
            "Epoch 207, Loss 0.124649\n",
            "Epoch 208, Loss 0.277300\n",
            "Epoch 209, Loss 0.176002\n",
            "Epoch 210, Loss 0.058759\n",
            "Epoch 211, Loss 0.090487\n",
            "Epoch 212, Loss 0.228045\n",
            "Epoch 213, Loss 0.229800\n",
            "Epoch 214, Loss 0.094841\n",
            "Epoch 215, Loss 0.163487\n",
            "Epoch 216, Loss 0.303484\n",
            "Epoch 217, Loss 0.065400\n",
            "Epoch 218, Loss 0.105992\n",
            "Epoch 219, Loss 0.073203\n",
            "Epoch 220, Loss 0.035824\n",
            "Epoch 221, Loss 0.307168\n",
            "Epoch 222, Loss 0.090701\n",
            "Epoch 223, Loss 0.038692\n",
            "Epoch 224, Loss 0.077532\n",
            "Epoch 225, Loss 0.041422\n",
            "Epoch 226, Loss 0.053518\n",
            "Epoch 227, Loss 0.083598\n",
            "Epoch 228, Loss 0.081853\n",
            "Epoch 229, Loss 0.152385\n",
            "Epoch 230, Loss 0.103338\n",
            "Epoch 231, Loss 0.050441\n",
            "Epoch 232, Loss 0.144752\n",
            "Epoch 233, Loss 0.090305\n",
            "Epoch 234, Loss 0.048403\n",
            "Epoch 235, Loss 0.075271\n",
            "Epoch 236, Loss 0.043134\n",
            "Epoch 237, Loss 0.079968\n",
            "Epoch 238, Loss 0.084964\n",
            "Epoch 239, Loss 0.057034\n",
            "Epoch 240, Loss 0.042415\n",
            "Epoch 241, Loss 0.030481\n",
            "Epoch 242, Loss 0.043832\n",
            "Epoch 243, Loss 0.033327\n",
            "Epoch 244, Loss 0.060116\n",
            "Epoch 245, Loss 0.039187\n",
            "Epoch 246, Loss 0.030242\n",
            "Epoch 247, Loss 0.023927\n",
            "Epoch 248, Loss 0.020571\n",
            "Epoch 249, Loss 0.032596\n",
            "Epoch 250, Loss 0.035041\n",
            "Epoch 251, Loss 0.042434\n",
            "Epoch 252, Loss 0.036036\n",
            "Epoch 253, Loss 0.064705\n",
            "Epoch 254, Loss 0.034757\n",
            "Epoch 255, Loss 0.023596\n",
            "Epoch 256, Loss 0.035125\n",
            "Epoch 257, Loss 0.182137\n",
            "Epoch 258, Loss 0.041553\n",
            "Epoch 259, Loss 0.023747\n",
            "Epoch 260, Loss 0.023479\n",
            "Epoch 261, Loss 0.021673\n",
            "Epoch 262, Loss 0.034434\n",
            "Epoch 263, Loss 0.025561\n",
            "Epoch 264, Loss 0.071351\n",
            "Epoch 265, Loss 0.049116\n",
            "Epoch 266, Loss 0.036087\n",
            "Epoch 267, Loss 0.018308\n",
            "Epoch 268, Loss 0.016597\n",
            "Epoch 269, Loss 0.031543\n",
            "Epoch 270, Loss 0.019323\n",
            "Epoch 271, Loss 0.023605\n",
            "Epoch 272, Loss 0.021587\n",
            "Epoch 273, Loss 0.018862\n",
            "Epoch 274, Loss 0.021823\n",
            "Epoch 275, Loss 0.034134\n",
            "Epoch 276, Loss 0.013755\n",
            "Epoch 277, Loss 0.013126\n",
            "Epoch 278, Loss 0.016714\n",
            "Epoch 279, Loss 0.011450\n",
            "Epoch 280, Loss 0.022118\n",
            "Epoch 281, Loss 0.022413\n",
            "Epoch 282, Loss 0.020563\n",
            "Epoch 283, Loss 0.016775\n",
            "Epoch 284, Loss 0.017779\n",
            "Epoch 285, Loss 0.021062\n",
            "Epoch 286, Loss 0.018662\n",
            "Epoch 287, Loss 0.008131\n",
            "Epoch 288, Loss 0.020512\n",
            "Epoch 289, Loss 0.010996\n",
            "Epoch 290, Loss 0.007940\n",
            "Epoch 291, Loss 0.016768\n",
            "Epoch 292, Loss 0.018240\n",
            "Epoch 293, Loss 0.027683\n",
            "Epoch 294, Loss 0.010147\n",
            "Epoch 295, Loss 0.019657\n",
            "Epoch 296, Loss 0.010426\n",
            "Epoch 297, Loss 0.011126\n",
            "Epoch 298, Loss 0.011483\n",
            "Epoch 299, Loss 0.010997\n"
          ]
        }
      ],
      "source": [
        "#1b - ANN - 3 hidden layers\n",
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Epoch %d, Loss %f' % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy for 1b - ANN - 3 hidden layers\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTGApSJ4lXhs",
        "outputId": "06f0e80b-4f91-441e-f9ab-57de291332de"
      },
      "id": "FTGApSJ4lXhs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.460000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4827efa5",
      "metadata": {
        "id": "4827efa5"
      },
      "outputs": [],
      "source": [
        "#define training loop\n",
        "import datetime  # <1>\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  # <2>\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # <3>           \n",
        "            outputs = model(imgs)  # <4>           \n",
        "            loss = loss_fn(outputs, labels)  # <5>\n",
        "            optimizer.zero_grad()  # <6>         \n",
        "            loss.backward()  # <7>         \n",
        "            optimizer.step()  # <8>\n",
        "            loss_train += loss.item()  # <9>\n",
        "\n",
        "        print('{} Epoch {}, Training loss {}'.format(\n",
        "        datetime.datetime.now(), epoch,\n",
        "        loss_train / len(train_loader)))  # <10>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define validation func\n",
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\n",
        "          batchsize = imgs.shape[0]\n",
        "          outputs = model(imgs)\n",
        "          _, predicted = torch.max(outputs, dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
      ],
      "metadata": {
        "id": "nYozGbkIVTUu"
      },
      "id": "nYozGbkIVTUu",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "888fe93e",
      "metadata": {
        "id": "888fe93e"
      },
      "outputs": [],
      "source": [
        "#2a - CNN - 2 layers - model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "efbb5c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "efbb5c00",
        "outputId": "88139ecd-1dae-4ee0-8b4f-3232d883d025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-09 22:05:26.465810 Epoch 1, Training loss 1.9919952079463188\n",
            "2022-12-09 22:06:09.736653 Epoch 2, Training loss 1.7604422360429983\n",
            "2022-12-09 22:06:48.099178 Epoch 3, Training loss 1.6112689374352964\n",
            "2022-12-09 22:07:28.230946 Epoch 4, Training loss 1.514346876870031\n",
            "2022-12-09 22:08:07.156433 Epoch 5, Training loss 1.4482414100481116\n",
            "2022-12-09 22:08:46.454274 Epoch 6, Training loss 1.3905521558068903\n",
            "2022-12-09 22:09:28.624908 Epoch 7, Training loss 1.3352689595173692\n",
            "2022-12-09 22:10:08.499994 Epoch 8, Training loss 1.2801164304813766\n",
            "2022-12-09 22:10:47.150999 Epoch 9, Training loss 1.2304648919331143\n",
            "2022-12-09 22:11:27.454509 Epoch 10, Training loss 1.1907181572121428\n",
            "2022-12-09 22:12:07.871888 Epoch 11, Training loss 1.1581725972082915\n",
            "2022-12-09 22:12:48.491309 Epoch 12, Training loss 1.13173695506952\n",
            "2022-12-09 22:13:30.351078 Epoch 13, Training loss 1.1081989951755689\n",
            "2022-12-09 22:14:08.910443 Epoch 14, Training loss 1.0862986528507583\n",
            "2022-12-09 22:14:47.742848 Epoch 15, Training loss 1.066214043999572\n",
            "2022-12-09 22:15:29.795807 Epoch 16, Training loss 1.0500644709905396\n",
            "2022-12-09 22:16:10.527428 Epoch 17, Training loss 1.0311424841966166\n",
            "2022-12-09 22:16:52.055168 Epoch 18, Training loss 1.0148440065896114\n",
            "2022-12-09 22:17:30.917527 Epoch 19, Training loss 1.0000099728022085\n",
            "2022-12-09 22:18:10.093563 Epoch 20, Training loss 0.9860043332095036\n",
            "2022-12-09 22:18:48.714921 Epoch 21, Training loss 0.9729916030336219\n",
            "2022-12-09 22:19:28.669196 Epoch 22, Training loss 0.9621287869370502\n",
            "2022-12-09 22:20:12.743190 Epoch 23, Training loss 0.9522113847305708\n",
            "2022-12-09 22:20:51.597070 Epoch 24, Training loss 0.9416722333644663\n",
            "2022-12-09 22:21:29.800889 Epoch 25, Training loss 0.9320103754777738\n",
            "2022-12-09 22:22:08.570377 Epoch 26, Training loss 0.924054097489018\n",
            "2022-12-09 22:22:46.164223 Epoch 27, Training loss 0.9160568816277682\n",
            "2022-12-09 22:23:25.412220 Epoch 28, Training loss 0.9080410683551408\n",
            "2022-12-09 22:24:06.568980 Epoch 29, Training loss 0.9010324808185363\n",
            "2022-12-09 22:24:44.800980 Epoch 30, Training loss 0.8937621922291759\n",
            "2022-12-09 22:25:23.182368 Epoch 31, Training loss 0.8860711876464926\n",
            "2022-12-09 22:26:01.973821 Epoch 32, Training loss 0.879986215392342\n",
            "2022-12-09 22:26:39.656949 Epoch 33, Training loss 0.8768790147036237\n",
            "2022-12-09 22:27:17.733520 Epoch 34, Training loss 0.870375247867516\n",
            "2022-12-09 22:27:58.773578 Epoch 35, Training loss 0.8628105857335698\n",
            "2022-12-09 22:28:38.345438 Epoch 36, Training loss 0.858736286497177\n",
            "2022-12-09 22:29:15.768441 Epoch 37, Training loss 0.8542259785982654\n",
            "2022-12-09 22:29:54.267740 Epoch 38, Training loss 0.8492973531451067\n",
            "2022-12-09 22:30:32.890298 Epoch 39, Training loss 0.843401609784197\n",
            "2022-12-09 22:31:11.361609 Epoch 40, Training loss 0.8398551385268531\n",
            "2022-12-09 22:31:53.655121 Epoch 41, Training loss 0.8341764089701426\n",
            "2022-12-09 22:32:30.754892 Epoch 42, Training loss 0.8304583067860445\n",
            "2022-12-09 22:33:07.823320 Epoch 43, Training loss 0.82678811827584\n",
            "2022-12-09 22:33:46.204368 Epoch 44, Training loss 0.8213872690792279\n",
            "2022-12-09 22:34:23.722353 Epoch 45, Training loss 0.8188410074738286\n",
            "2022-12-09 22:35:03.768602 Epoch 46, Training loss 0.8152564698472962\n",
            "2022-12-09 22:35:41.374924 Epoch 47, Training loss 0.8113594905799612\n",
            "2022-12-09 22:36:19.607583 Epoch 48, Training loss 0.8083517307515644\n",
            "2022-12-09 22:36:56.374128 Epoch 49, Training loss 0.8029714324666412\n",
            "2022-12-09 22:37:33.918708 Epoch 50, Training loss 0.799939693773494\n",
            "2022-12-09 22:38:11.906546 Epoch 51, Training loss 0.7970114195971842\n",
            "2022-12-09 22:38:52.812489 Epoch 52, Training loss 0.7943510977203584\n",
            "2022-12-09 22:39:29.675143 Epoch 53, Training loss 0.7904090169445633\n",
            "2022-12-09 22:40:07.124746 Epoch 54, Training loss 0.7881705451499471\n",
            "2022-12-09 22:40:44.088063 Epoch 55, Training loss 0.7850923430736717\n",
            "2022-12-09 22:41:21.148513 Epoch 56, Training loss 0.783022630397621\n",
            "2022-12-09 22:41:59.446320 Epoch 57, Training loss 0.7787174111437005\n",
            "2022-12-09 22:42:39.404291 Epoch 58, Training loss 0.7757324569136895\n",
            "2022-12-09 22:43:16.921555 Epoch 59, Training loss 0.7742251298769051\n",
            "2022-12-09 22:43:55.417308 Epoch 60, Training loss 0.7706346343011807\n",
            "2022-12-09 22:44:32.380280 Epoch 61, Training loss 0.7675095925779294\n",
            "2022-12-09 22:45:09.416016 Epoch 62, Training loss 0.765592431663857\n",
            "2022-12-09 22:45:49.304550 Epoch 63, Training loss 0.7618284991101536\n",
            "2022-12-09 22:46:27.558808 Epoch 64, Training loss 0.7602776140736802\n",
            "2022-12-09 22:47:04.928133 Epoch 65, Training loss 0.7577651666329644\n",
            "2022-12-09 22:47:41.975100 Epoch 66, Training loss 0.7536482633760823\n",
            "2022-12-09 22:48:19.173837 Epoch 67, Training loss 0.7521861617827355\n",
            "2022-12-09 22:48:57.472996 Epoch 68, Training loss 0.750208023945084\n",
            "2022-12-09 22:49:38.305523 Epoch 69, Training loss 0.7470302332164077\n",
            "2022-12-09 22:50:15.650723 Epoch 70, Training loss 0.7444543052664803\n",
            "2022-12-09 22:50:52.361147 Epoch 71, Training loss 0.742271930932084\n",
            "2022-12-09 22:51:30.708283 Epoch 72, Training loss 0.7406570476186854\n",
            "2022-12-09 22:52:07.681624 Epoch 73, Training loss 0.7383163298487359\n",
            "2022-12-09 22:52:47.422383 Epoch 74, Training loss 0.7349598248443945\n",
            "2022-12-09 22:53:24.237619 Epoch 75, Training loss 0.7337509787753415\n",
            "2022-12-09 22:54:02.520468 Epoch 76, Training loss 0.732107063236139\n",
            "2022-12-09 22:54:39.528245 Epoch 77, Training loss 0.7278738212021415\n",
            "2022-12-09 22:55:16.209990 Epoch 78, Training loss 0.727285390162407\n",
            "2022-12-09 22:55:53.906148 Epoch 79, Training loss 0.72382601215254\n",
            "2022-12-09 22:56:34.991429 Epoch 80, Training loss 0.7224230445025827\n",
            "2022-12-09 22:57:11.849067 Epoch 81, Training loss 0.7211028878265024\n",
            "2022-12-09 22:57:48.980899 Epoch 82, Training loss 0.7183062089678577\n",
            "2022-12-09 22:58:26.200185 Epoch 83, Training loss 0.717023577302923\n",
            "2022-12-09 22:59:04.750822 Epoch 84, Training loss 0.7141725625223516\n",
            "2022-12-09 22:59:45.320366 Epoch 85, Training loss 0.7115780179153013\n",
            "2022-12-09 23:00:22.801744 Epoch 86, Training loss 0.7114886944479955\n",
            "2022-12-09 23:01:00.233823 Epoch 87, Training loss 0.7093881074424899\n",
            "2022-12-09 23:01:38.465825 Epoch 88, Training loss 0.7081851627287048\n",
            "2022-12-09 23:02:16.240320 Epoch 89, Training loss 0.7056103434099261\n",
            "2022-12-09 23:02:54.151861 Epoch 90, Training loss 0.7020804781819243\n",
            "2022-12-09 23:03:35.069970 Epoch 91, Training loss 0.7019041045318783\n",
            "2022-12-09 23:04:12.106017 Epoch 92, Training loss 0.6992709537220123\n",
            "2022-12-09 23:04:48.890487 Epoch 93, Training loss 0.6986992203671\n",
            "2022-12-09 23:05:26.194431 Epoch 94, Training loss 0.6959914851676473\n",
            "2022-12-09 23:06:03.234997 Epoch 95, Training loss 0.6951166910436147\n",
            "2022-12-09 23:06:43.469396 Epoch 96, Training loss 0.6921905907599822\n",
            "2022-12-09 23:07:22.570307 Epoch 97, Training loss 0.6914647742534232\n",
            "2022-12-09 23:08:00.124365 Epoch 98, Training loss 0.690055772281059\n",
            "2022-12-09 23:08:37.243315 Epoch 99, Training loss 0.6868784569413461\n",
            "2022-12-09 23:09:15.776912 Epoch 100, Training loss 0.6849311193084473\n",
            "2022-12-09 23:09:53.309838 Epoch 101, Training loss 0.6836506527326905\n",
            "2022-12-09 23:10:33.747886 Epoch 102, Training loss 0.6825984257947454\n",
            "2022-12-09 23:11:10.794310 Epoch 103, Training loss 0.6808463363238918\n",
            "2022-12-09 23:11:49.084661 Epoch 104, Training loss 0.6782884832157199\n",
            "2022-12-09 23:12:26.330382 Epoch 105, Training loss 0.6766547561072938\n",
            "2022-12-09 23:13:03.671589 Epoch 106, Training loss 0.6765550586878492\n",
            "2022-12-09 23:13:45.141510 Epoch 107, Training loss 0.6743081440324978\n",
            "2022-12-09 23:14:22.233818 Epoch 108, Training loss 0.6734082779234938\n",
            "2022-12-09 23:14:59.573083 Epoch 109, Training loss 0.6717205258738964\n",
            "2022-12-09 23:15:36.712358 Epoch 110, Training loss 0.6689116778352376\n",
            "2022-12-09 23:16:13.616061 Epoch 111, Training loss 0.668914377346368\n",
            "2022-12-09 23:16:51.987968 Epoch 112, Training loss 0.6671178324905502\n",
            "2022-12-09 23:17:32.010184 Epoch 113, Training loss 0.6650333332512385\n",
            "2022-12-09 23:18:09.836748 Epoch 114, Training loss 0.6641160972283014\n",
            "2022-12-09 23:18:47.229430 Epoch 115, Training loss 0.6630450089645508\n",
            "2022-12-09 23:19:26.052626 Epoch 116, Training loss 0.6616324217194487\n",
            "2022-12-09 23:20:03.375013 Epoch 117, Training loss 0.6604624240828292\n",
            "2022-12-09 23:20:43.505672 Epoch 118, Training loss 0.6592620193500958\n",
            "2022-12-09 23:21:21.654157 Epoch 119, Training loss 0.6565293218854749\n",
            "2022-12-09 23:21:58.791634 Epoch 120, Training loss 0.6549584207022586\n",
            "2022-12-09 23:22:35.694396 Epoch 121, Training loss 0.6548524212349406\n",
            "2022-12-09 23:23:12.901546 Epoch 122, Training loss 0.6540716609457875\n",
            "2022-12-09 23:23:50.963136 Epoch 123, Training loss 0.6523299040773031\n",
            "2022-12-09 23:24:31.527263 Epoch 124, Training loss 0.6500295011131355\n",
            "2022-12-09 23:25:08.725499 Epoch 125, Training loss 0.6481797828927369\n",
            "2022-12-09 23:25:46.009629 Epoch 126, Training loss 0.6483873068295476\n",
            "2022-12-09 23:26:24.363178 Epoch 127, Training loss 0.6464092732238038\n",
            "2022-12-09 23:27:01.227130 Epoch 128, Training loss 0.6451884474214691\n",
            "2022-12-09 23:27:41.706219 Epoch 129, Training loss 0.644786073576154\n",
            "2022-12-09 23:28:18.857285 Epoch 130, Training loss 0.6419892149889256\n",
            "2022-12-09 23:28:57.046907 Epoch 131, Training loss 0.641742573522241\n",
            "2022-12-09 23:29:34.137834 Epoch 132, Training loss 0.6399849343406575\n",
            "2022-12-09 23:30:11.497990 Epoch 133, Training loss 0.6392688511125267\n",
            "2022-12-09 23:30:48.301055 Epoch 134, Training loss 0.6374563562214527\n",
            "2022-12-09 23:31:29.735146 Epoch 135, Training loss 0.6370735549179795\n",
            "2022-12-09 23:32:07.245600 Epoch 136, Training loss 0.634761916516382\n",
            "2022-12-09 23:32:44.601516 Epoch 137, Training loss 0.6343607513800912\n",
            "2022-12-09 23:33:22.001581 Epoch 138, Training loss 0.6323956052208191\n",
            "2022-12-09 23:33:59.903299 Epoch 139, Training loss 0.6322750975103939\n",
            "2022-12-09 23:34:37.156445 Epoch 140, Training loss 0.6309719144383354\n",
            "2022-12-09 23:35:14.260033 Epoch 141, Training loss 0.6288091400090385\n",
            "2022-12-09 23:35:55.717800 Epoch 142, Training loss 0.6292556261315065\n",
            "2022-12-09 23:36:33.183761 Epoch 143, Training loss 0.6278139610043572\n",
            "2022-12-09 23:37:10.159880 Epoch 144, Training loss 0.6268827813436918\n",
            "2022-12-09 23:37:47.289537 Epoch 145, Training loss 0.6248815025743621\n",
            "2022-12-09 23:38:26.504346 Epoch 146, Training loss 0.6240251824983856\n",
            "2022-12-09 23:39:06.738567 Epoch 147, Training loss 0.6230913889606285\n",
            "2022-12-09 23:39:43.976852 Epoch 148, Training loss 0.6240192414702052\n",
            "2022-12-09 23:40:22.694463 Epoch 149, Training loss 0.6223471598018466\n",
            "2022-12-09 23:40:59.878422 Epoch 150, Training loss 0.6189892822519287\n",
            "2022-12-09 23:41:37.236087 Epoch 151, Training loss 0.6170903411134124\n",
            "2022-12-09 23:42:16.337607 Epoch 152, Training loss 0.6178900710762004\n",
            "2022-12-09 23:42:56.324577 Epoch 153, Training loss 0.6167644232587741\n",
            "2022-12-09 23:43:33.446347 Epoch 154, Training loss 0.6158985521482385\n",
            "2022-12-09 23:44:12.362156 Epoch 155, Training loss 0.6165643848878953\n",
            "2022-12-09 23:44:51.152685 Epoch 156, Training loss 0.6147316577260756\n",
            "2022-12-09 23:45:29.606489 Epoch 157, Training loss 0.6129541916920401\n",
            "2022-12-09 23:46:06.479391 Epoch 158, Training loss 0.6125900791505413\n",
            "2022-12-09 23:46:45.264516 Epoch 159, Training loss 0.6110802667829996\n",
            "2022-12-09 23:47:24.979195 Epoch 160, Training loss 0.6092495533740124\n",
            "2022-12-09 23:48:02.155428 Epoch 161, Training loss 0.609093350484548\n",
            "2022-12-09 23:48:40.623296 Epoch 162, Training loss 0.6084686146901391\n",
            "2022-12-09 23:49:18.516717 Epoch 163, Training loss 0.6071427316997972\n",
            "2022-12-09 23:49:57.534552 Epoch 164, Training loss 0.6058220313028302\n",
            "2022-12-09 23:50:34.669744 Epoch 165, Training loss 0.6057403805615652\n",
            "2022-12-09 23:51:13.095406 Epoch 166, Training loss 0.6042331318416254\n",
            "2022-12-09 23:51:50.495221 Epoch 167, Training loss 0.6035583903417563\n",
            "2022-12-09 23:52:30.514574 Epoch 168, Training loss 0.6019879453398688\n",
            "2022-12-09 23:53:07.558471 Epoch 169, Training loss 0.6024569829025537\n",
            "2022-12-09 23:53:45.946510 Epoch 170, Training loss 0.5996800331622744\n",
            "2022-12-09 23:54:23.427942 Epoch 171, Training loss 0.5984851159631749\n",
            "2022-12-09 23:55:02.200756 Epoch 172, Training loss 0.5994427110380529\n",
            "2022-12-09 23:55:40.198990 Epoch 173, Training loss 0.5991928487101479\n",
            "2022-12-09 23:56:17.127216 Epoch 174, Training loss 0.5986076392176206\n",
            "2022-12-09 23:56:55.162309 Epoch 175, Training loss 0.5976194411211306\n",
            "2022-12-09 23:57:32.846797 Epoch 176, Training loss 0.5960689447343807\n",
            "2022-12-09 23:58:10.518234 Epoch 177, Training loss 0.5958870987376899\n",
            "2022-12-09 23:58:47.029195 Epoch 178, Training loss 0.5945837764864992\n",
            "2022-12-09 23:59:25.499052 Epoch 179, Training loss 0.5920928328695809\n",
            "2022-12-10 00:00:05.748554 Epoch 180, Training loss 0.5910367398615688\n",
            "2022-12-10 00:00:43.750382 Epoch 181, Training loss 0.5915976260857814\n",
            "2022-12-10 00:01:21.242263 Epoch 182, Training loss 0.5909256508664402\n",
            "2022-12-10 00:02:00.707578 Epoch 183, Training loss 0.5903202701178963\n",
            "2022-12-10 00:02:39.008483 Epoch 184, Training loss 0.5887637044043492\n",
            "2022-12-10 00:03:16.491475 Epoch 185, Training loss 0.5884610307414818\n",
            "2022-12-10 00:03:54.358148 Epoch 186, Training loss 0.5874912468216303\n",
            "2022-12-10 00:04:33.802300 Epoch 187, Training loss 0.5854549857280443\n",
            "2022-12-10 00:05:11.619040 Epoch 188, Training loss 0.5864353320558967\n",
            "2022-12-10 00:05:48.514441 Epoch 189, Training loss 0.5853045637269154\n",
            "2022-12-10 00:06:27.930111 Epoch 190, Training loss 0.5840018314245107\n",
            "2022-12-10 00:07:07.077228 Epoch 191, Training loss 0.5824207715366198\n",
            "2022-12-10 00:07:44.077437 Epoch 192, Training loss 0.583150246983294\n",
            "2022-12-10 00:08:22.237759 Epoch 193, Training loss 0.5842852580654042\n",
            "2022-12-10 00:09:01.332314 Epoch 194, Training loss 0.5822620173854292\n",
            "2022-12-10 00:09:39.189730 Epoch 195, Training loss 0.5810004661378958\n",
            "2022-12-10 00:10:16.037493 Epoch 196, Training loss 0.5798373965122511\n",
            "2022-12-10 00:10:53.840732 Epoch 197, Training loss 0.57891871229462\n",
            "2022-12-10 00:11:33.511844 Epoch 198, Training loss 0.5791121594741216\n",
            "2022-12-10 00:12:11.046055 Epoch 199, Training loss 0.5784181748585933\n",
            "2022-12-10 00:12:49.175847 Epoch 200, Training loss 0.578176823266022\n",
            "2022-12-10 00:13:27.118648 Epoch 201, Training loss 0.5770662036507636\n",
            "2022-12-10 00:14:07.076771 Epoch 202, Training loss 0.575524640281487\n",
            "2022-12-10 00:14:43.830798 Epoch 203, Training loss 0.5748306760550155\n",
            "2022-12-10 00:15:21.633740 Epoch 204, Training loss 0.5747374155179924\n",
            "2022-12-10 00:16:00.771063 Epoch 205, Training loss 0.5735102273009317\n",
            "2022-12-10 00:16:38.898384 Epoch 206, Training loss 0.5732729987377096\n",
            "2022-12-10 00:17:16.920335 Epoch 207, Training loss 0.572466131442648\n",
            "2022-12-10 00:17:53.734411 Epoch 208, Training loss 0.5732359617110103\n",
            "2022-12-10 00:18:33.102035 Epoch 209, Training loss 0.5718101418536642\n",
            "2022-12-10 00:19:10.307063 Epoch 210, Training loss 0.571350875070028\n",
            "2022-12-10 00:19:48.227386 Epoch 211, Training loss 0.5705723953254692\n",
            "2022-12-10 00:20:25.915095 Epoch 212, Training loss 0.567853437817615\n",
            "2022-12-10 00:21:04.914883 Epoch 213, Training loss 0.570048760689433\n",
            "2022-12-10 00:21:43.065736 Epoch 214, Training loss 0.5666080130564283\n",
            "2022-12-10 00:22:19.758490 Epoch 215, Training loss 0.5670369198483884\n",
            "2022-12-10 00:22:58.162640 Epoch 216, Training loss 0.5670843074083938\n",
            "2022-12-10 00:23:37.158143 Epoch 217, Training loss 0.566275233998323\n",
            "2022-12-10 00:24:15.135864 Epoch 218, Training loss 0.564551178337363\n",
            "2022-12-10 00:24:53.235034 Epoch 219, Training loss 0.5633988340797327\n",
            "2022-12-10 00:25:32.093808 Epoch 220, Training loss 0.5656593891284655\n",
            "2022-12-10 00:26:09.513977 Epoch 221, Training loss 0.5642708070442805\n",
            "2022-12-10 00:26:46.392241 Epoch 222, Training loss 0.5635548093168022\n",
            "2022-12-10 00:27:24.394582 Epoch 223, Training loss 0.5625191168940585\n",
            "2022-12-10 00:28:03.814346 Epoch 224, Training loss 0.5627648377662424\n",
            "2022-12-10 00:28:42.108579 Epoch 225, Training loss 0.5623405755633284\n",
            "2022-12-10 00:29:20.191646 Epoch 226, Training loss 0.5594137319366036\n",
            "2022-12-10 00:29:57.850236 Epoch 227, Training loss 0.559690445280441\n",
            "2022-12-10 00:30:35.492650 Epoch 228, Training loss 0.559908726121611\n",
            "2022-12-10 00:31:13.139105 Epoch 229, Training loss 0.5590659189597725\n",
            "2022-12-10 00:31:50.986704 Epoch 230, Training loss 0.5568528928415245\n",
            "2022-12-10 00:32:30.396311 Epoch 231, Training loss 0.5583455430729615\n",
            "2022-12-10 00:33:08.129569 Epoch 232, Training loss 0.55674596729181\n",
            "2022-12-10 00:33:46.034526 Epoch 233, Training loss 0.5564854315022374\n",
            "2022-12-10 00:34:22.519556 Epoch 234, Training loss 0.5585987821335683\n",
            "2022-12-10 00:35:01.310449 Epoch 235, Training loss 0.555387789940895\n",
            "2022-12-10 00:35:39.283242 Epoch 236, Training loss 0.5555881377871689\n",
            "2022-12-10 00:36:17.116179 Epoch 237, Training loss 0.5541174561547502\n",
            "2022-12-10 00:36:54.873198 Epoch 238, Training loss 0.5535376252573164\n",
            "2022-12-10 00:37:34.105370 Epoch 239, Training loss 0.5535184630118978\n",
            "2022-12-10 00:38:11.959765 Epoch 240, Training loss 0.55105799021166\n",
            "2022-12-10 00:38:48.250303 Epoch 241, Training loss 0.5518794036506082\n",
            "2022-12-10 00:39:26.750407 Epoch 242, Training loss 0.5520860483045773\n",
            "2022-12-10 00:40:04.150737 Epoch 243, Training loss 0.5516336573969067\n",
            "2022-12-10 00:40:41.782162 Epoch 244, Training loss 0.5501448603161155\n",
            "2022-12-10 00:41:19.304121 Epoch 245, Training loss 0.5512103242299441\n",
            "2022-12-10 00:41:57.456307 Epoch 246, Training loss 0.5496475862343903\n",
            "2022-12-10 00:42:36.240921 Epoch 247, Training loss 0.5488638555073677\n",
            "2022-12-10 00:43:12.646016 Epoch 248, Training loss 0.5489724197655993\n",
            "2022-12-10 00:43:51.425218 Epoch 249, Training loss 0.5487072515060835\n",
            "2022-12-10 00:44:29.161602 Epoch 250, Training loss 0.5473006170271607\n",
            "2022-12-10 00:45:06.871985 Epoch 251, Training loss 0.546826968507846\n",
            "2022-12-10 00:45:44.760652 Epoch 252, Training loss 0.5475965564703698\n",
            "2022-12-10 00:46:22.977188 Epoch 253, Training loss 0.5466189058235539\n",
            "2022-12-10 00:47:02.291788 Epoch 254, Training loss 0.5455038726634687\n",
            "2022-12-10 00:47:39.119618 Epoch 255, Training loss 0.5455156314708388\n",
            "2022-12-10 00:48:18.015192 Epoch 256, Training loss 0.5437153359813154\n",
            "2022-12-10 00:48:55.611452 Epoch 257, Training loss 0.5446884521971578\n",
            "2022-12-10 00:49:33.083207 Epoch 258, Training loss 0.5435433541341206\n",
            "2022-12-10 00:50:11.047782 Epoch 259, Training loss 0.5417296663117226\n",
            "2022-12-10 00:50:48.809700 Epoch 260, Training loss 0.5433695809844205\n",
            "2022-12-10 00:51:27.745228 Epoch 261, Training loss 0.543733682664459\n",
            "2022-12-10 00:52:05.311505 Epoch 262, Training loss 0.542637387123864\n",
            "2022-12-10 00:52:43.021211 Epoch 263, Training loss 0.5421467634951672\n",
            "2022-12-10 00:53:20.585716 Epoch 264, Training loss 0.541845641325197\n",
            "2022-12-10 00:53:58.365133 Epoch 265, Training loss 0.5421330510540996\n",
            "2022-12-10 00:54:35.996722 Epoch 266, Training loss 0.5394517095459391\n",
            "2022-12-10 00:55:13.541430 Epoch 267, Training loss 0.5398314506806376\n",
            "2022-12-10 00:55:52.430593 Epoch 268, Training loss 0.5404691391283899\n",
            "2022-12-10 00:56:30.305766 Epoch 269, Training loss 0.5395169073472852\n",
            "2022-12-10 00:57:07.829094 Epoch 270, Training loss 0.5395601002875802\n",
            "2022-12-10 00:57:45.729825 Epoch 271, Training loss 0.5369795733476843\n",
            "2022-12-10 00:58:23.767411 Epoch 272, Training loss 0.5372954409788636\n",
            "2022-12-10 00:59:01.423862 Epoch 273, Training loss 0.5357711532574785\n",
            "2022-12-10 00:59:40.260129 Epoch 274, Training loss 0.5356231465211609\n",
            "2022-12-10 01:00:18.262716 Epoch 275, Training loss 0.5376084629836899\n",
            "2022-12-10 01:00:55.955994 Epoch 276, Training loss 0.5362157647490806\n",
            "2022-12-10 01:01:33.817024 Epoch 277, Training loss 0.5365773022098614\n",
            "2022-12-10 01:02:11.621516 Epoch 278, Training loss 0.5355843433257564\n",
            "2022-12-10 01:02:49.531456 Epoch 279, Training loss 0.5356803961345912\n",
            "2022-12-10 01:03:27.257774 Epoch 280, Training loss 0.5347180671208654\n",
            "2022-12-10 01:04:06.142930 Epoch 281, Training loss 0.5326640446243993\n",
            "2022-12-10 01:04:44.158805 Epoch 282, Training loss 0.5339323948411381\n",
            "2022-12-10 01:05:21.996343 Epoch 283, Training loss 0.5323534622178663\n",
            "2022-12-10 01:06:00.205829 Epoch 284, Training loss 0.534504791335834\n",
            "2022-12-10 01:06:38.636133 Epoch 285, Training loss 0.5315644834428797\n",
            "2022-12-10 01:07:16.834634 Epoch 286, Training loss 0.5322170766723126\n",
            "2022-12-10 01:07:55.909700 Epoch 287, Training loss 0.5308471429912026\n",
            "2022-12-10 01:08:33.895874 Epoch 288, Training loss 0.5310943843915944\n",
            "2022-12-10 01:09:11.570165 Epoch 289, Training loss 0.5307149622598877\n",
            "2022-12-10 01:09:48.981317 Epoch 290, Training loss 0.5303378572778019\n",
            "2022-12-10 01:10:26.936718 Epoch 291, Training loss 0.5310784340133448\n",
            "2022-12-10 01:11:06.473602 Epoch 292, Training loss 0.529879819630357\n",
            "2022-12-10 01:11:44.543718 Epoch 293, Training loss 0.5312815134215843\n",
            "2022-12-10 01:12:22.342477 Epoch 294, Training loss 0.5293354386335138\n",
            "2022-12-10 01:13:00.205972 Epoch 295, Training loss 0.528114362674601\n",
            "2022-12-10 01:13:37.973998 Epoch 296, Training loss 0.5285231893324791\n",
            "2022-12-10 01:14:15.815054 Epoch 297, Training loss 0.5278964528189901\n",
            "2022-12-10 01:14:53.505004 Epoch 298, Training loss 0.5280852538088093\n",
            "2022-12-10 01:15:32.600949 Epoch 299, Training loss 0.5261223490166542\n",
            "2022-12-10 01:16:10.642930 Epoch 300, Training loss 0.5247760035116654\n"
          ]
        }
      ],
      "source": [
        "#2a - CNN - 2 layers\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy for 2a - CNN - 2 layers\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sf2kqVOeWUOw",
        "outputId": "bf75138f-73b1-4384-c633-8024ca7e9aed"
      },
      "id": "Sf2kqVOeWUOw",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.80\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8a65e0cd",
      "metadata": {
        "id": "8a65e0cd"
      },
      "outputs": [],
      "source": [
        "#2b - CNN - 3 layers - model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2b - CNN - 3 layers\n",
        "model = Net()  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "zNd1-T3jXIg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "13f087cb-ef93-4ab8-84d7-777037c68de0"
      },
      "id": "zNd1-T3jXIg7",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-10 02:44:03.941928 Epoch 1, Training loss 2.0370713297058556\n",
            "2022-12-10 02:44:40.747461 Epoch 2, Training loss 1.7988238109042272\n",
            "2022-12-10 02:45:18.633049 Epoch 3, Training loss 1.6391738330006904\n",
            "2022-12-10 02:45:55.218282 Epoch 4, Training loss 1.5354142152439907\n",
            "2022-12-10 02:46:30.611803 Epoch 5, Training loss 1.4516489928030907\n",
            "2022-12-10 02:47:08.585089 Epoch 6, Training loss 1.3753064373874908\n",
            "2022-12-10 02:47:45.296548 Epoch 7, Training loss 1.31402589270221\n",
            "2022-12-10 02:48:21.545227 Epoch 8, Training loss 1.2643911033640127\n",
            "2022-12-10 02:48:58.170818 Epoch 9, Training loss 1.2207249360316246\n",
            "2022-12-10 02:49:34.904904 Epoch 10, Training loss 1.1857656455405838\n",
            "2022-12-10 02:50:11.702748 Epoch 11, Training loss 1.1572102086470866\n",
            "2022-12-10 02:50:48.828220 Epoch 12, Training loss 1.1328304805566587\n",
            "2022-12-10 02:51:25.927992 Epoch 13, Training loss 1.1116205379176323\n",
            "2022-12-10 02:52:03.723788 Epoch 14, Training loss 1.094123426377011\n",
            "2022-12-10 02:52:40.818359 Epoch 15, Training loss 1.0769817492236262\n",
            "2022-12-10 02:53:16.390604 Epoch 16, Training loss 1.0618310281077918\n",
            "2022-12-10 02:53:53.663026 Epoch 17, Training loss 1.0482875175793152\n",
            "2022-12-10 02:54:30.686412 Epoch 18, Training loss 1.0353200186396498\n",
            "2022-12-10 02:55:07.141113 Epoch 19, Training loss 1.025427170193104\n",
            "2022-12-10 02:55:43.058376 Epoch 20, Training loss 1.0147264774345681\n",
            "2022-12-10 02:56:20.792278 Epoch 21, Training loss 1.0040565377001263\n",
            "2022-12-10 02:56:57.402655 Epoch 22, Training loss 0.9966415379510816\n",
            "2022-12-10 02:57:33.917042 Epoch 23, Training loss 0.9862443710227147\n",
            "2022-12-10 02:58:10.539034 Epoch 24, Training loss 0.9775372603360344\n",
            "2022-12-10 02:58:47.249465 Epoch 25, Training loss 0.9688650707683295\n",
            "2022-12-10 02:59:24.025857 Epoch 26, Training loss 0.9624824097851659\n",
            "2022-12-10 03:00:00.201652 Epoch 27, Training loss 0.9570929504111599\n",
            "2022-12-10 03:00:36.917944 Epoch 28, Training loss 0.9484160123273845\n",
            "2022-12-10 03:01:14.096082 Epoch 29, Training loss 0.9426241789937324\n",
            "2022-12-10 03:01:51.413029 Epoch 30, Training loss 0.9360235779334212\n",
            "2022-12-10 03:02:26.700161 Epoch 31, Training loss 0.9299073299331129\n",
            "2022-12-10 03:03:04.456373 Epoch 32, Training loss 0.924662909696779\n",
            "2022-12-10 03:03:40.647437 Epoch 33, Training loss 0.917498969513437\n",
            "2022-12-10 03:04:17.330473 Epoch 34, Training loss 0.9111606561009536\n",
            "2022-12-10 03:04:53.774374 Epoch 35, Training loss 0.9055763732289415\n",
            "2022-12-10 03:05:30.378766 Epoch 36, Training loss 0.9001410476234563\n",
            "2022-12-10 03:06:06.733690 Epoch 37, Training loss 0.8928811443431298\n",
            "2022-12-10 03:06:43.148429 Epoch 38, Training loss 0.8863794874504703\n",
            "2022-12-10 03:07:19.698154 Epoch 39, Training loss 0.88224900424328\n",
            "2022-12-10 03:07:56.271595 Epoch 40, Training loss 0.8767955547098614\n",
            "2022-12-10 03:08:33.835881 Epoch 41, Training loss 0.8718094964466436\n",
            "2022-12-10 03:09:09.600289 Epoch 42, Training loss 0.8666780116155629\n",
            "2022-12-10 03:09:46.269496 Epoch 43, Training loss 0.8607379339844979\n",
            "2022-12-10 03:10:23.880206 Epoch 44, Training loss 0.8556958214401285\n",
            "2022-12-10 03:11:00.407818 Epoch 45, Training loss 0.8509990780630989\n",
            "2022-12-10 03:11:35.847136 Epoch 46, Training loss 0.8466790047524225\n",
            "2022-12-10 03:12:13.929784 Epoch 47, Training loss 0.84178649205381\n",
            "2022-12-10 03:12:50.603447 Epoch 48, Training loss 0.8388256308291574\n",
            "2022-12-10 03:13:27.213699 Epoch 49, Training loss 0.8325835675610911\n",
            "2022-12-10 03:14:03.662047 Epoch 50, Training loss 0.8304753652618974\n",
            "2022-12-10 03:14:39.904434 Epoch 51, Training loss 0.8241621857256536\n",
            "2022-12-10 03:15:16.046810 Epoch 52, Training loss 0.8205172941660333\n",
            "2022-12-10 03:15:52.096659 Epoch 53, Training loss 0.8190058516648114\n",
            "2022-12-10 03:16:28.264249 Epoch 54, Training loss 0.8142770395025878\n",
            "2022-12-10 03:17:04.448173 Epoch 55, Training loss 0.8103845672458029\n",
            "2022-12-10 03:17:41.370106 Epoch 56, Training loss 0.8077008114065356\n",
            "2022-12-10 03:18:16.883191 Epoch 57, Training loss 0.8033890554590908\n",
            "2022-12-10 03:18:52.936797 Epoch 58, Training loss 0.7994286796016157\n",
            "2022-12-10 03:19:30.197965 Epoch 59, Training loss 0.7961940603411716\n",
            "2022-12-10 03:20:06.368667 Epoch 60, Training loss 0.7950097478716575\n",
            "2022-12-10 03:20:41.492528 Epoch 61, Training loss 0.7899074006415999\n",
            "2022-12-10 03:21:19.230995 Epoch 62, Training loss 0.7862225055618359\n",
            "2022-12-10 03:21:55.705643 Epoch 63, Training loss 0.7847147977260678\n",
            "2022-12-10 03:22:31.951733 Epoch 64, Training loss 0.782144735040872\n",
            "2022-12-10 03:23:08.275799 Epoch 65, Training loss 0.7790348223408164\n",
            "2022-12-10 03:23:44.282626 Epoch 66, Training loss 0.7757882734622492\n",
            "2022-12-10 03:24:20.549186 Epoch 67, Training loss 0.7732244456363151\n",
            "2022-12-10 03:24:56.980685 Epoch 68, Training loss 0.770350433371561\n",
            "2022-12-10 03:25:33.365356 Epoch 69, Training loss 0.7670043680597755\n",
            "2022-12-10 03:26:09.664818 Epoch 70, Training loss 0.7626811898577853\n",
            "2022-12-10 03:26:47.191692 Epoch 71, Training loss 0.7617542212805175\n",
            "2022-12-10 03:27:22.362627 Epoch 72, Training loss 0.7583517051490066\n",
            "2022-12-10 03:27:58.582706 Epoch 73, Training loss 0.7566813005663245\n",
            "2022-12-10 03:28:35.911137 Epoch 74, Training loss 0.7547264657819363\n",
            "2022-12-10 03:29:12.255598 Epoch 75, Training loss 0.7520693884328808\n",
            "2022-12-10 03:29:48.258624 Epoch 76, Training loss 0.749935259904398\n",
            "2022-12-10 03:30:24.487831 Epoch 77, Training loss 0.7462243889375111\n",
            "2022-12-10 03:31:00.592282 Epoch 78, Training loss 0.744815445235928\n",
            "2022-12-10 03:31:36.543479 Epoch 79, Training loss 0.7415188913378874\n",
            "2022-12-10 03:32:12.449486 Epoch 80, Training loss 0.7396937359476943\n",
            "2022-12-10 03:32:48.454072 Epoch 81, Training loss 0.7367011309432252\n",
            "2022-12-10 03:33:24.784306 Epoch 82, Training loss 0.736018282754342\n",
            "2022-12-10 03:34:01.121870 Epoch 83, Training loss 0.7347914951536662\n",
            "2022-12-10 03:34:37.436601 Epoch 84, Training loss 0.7301684358083379\n",
            "2022-12-10 03:35:13.808410 Epoch 85, Training loss 0.7290132511835878\n",
            "2022-12-10 03:35:50.121340 Epoch 86, Training loss 0.7260121047649237\n",
            "2022-12-10 03:36:26.295033 Epoch 87, Training loss 0.7262945802848967\n",
            "2022-12-10 03:37:02.416557 Epoch 88, Training loss 0.722588677388018\n",
            "2022-12-10 03:37:38.269511 Epoch 89, Training loss 0.7208955036023693\n",
            "2022-12-10 03:38:15.790930 Epoch 90, Training loss 0.7200329046496345\n",
            "2022-12-10 03:38:50.877733 Epoch 91, Training loss 0.7179755851283403\n",
            "2022-12-10 03:39:27.177875 Epoch 92, Training loss 0.7154723071228818\n",
            "2022-12-10 03:40:03.426453 Epoch 93, Training loss 0.712525931046442\n",
            "2022-12-10 03:40:40.872144 Epoch 94, Training loss 0.7118997976679327\n",
            "2022-12-10 03:41:15.716089 Epoch 95, Training loss 0.7108277525285931\n",
            "2022-12-10 03:41:53.145265 Epoch 96, Training loss 0.7074511760793378\n",
            "2022-12-10 03:42:28.045089 Epoch 97, Training loss 0.7053348699494091\n",
            "2022-12-10 03:43:05.494641 Epoch 98, Training loss 0.7042872616092263\n",
            "2022-12-10 03:43:41.766468 Epoch 99, Training loss 0.702792782536553\n",
            "2022-12-10 03:44:17.884692 Epoch 100, Training loss 0.7014092376546177\n",
            "2022-12-10 03:44:53.963446 Epoch 101, Training loss 0.6993379551736291\n",
            "2022-12-10 03:45:30.012881 Epoch 102, Training loss 0.6981236148444588\n",
            "2022-12-10 03:46:06.088086 Epoch 103, Training loss 0.6955024688063985\n",
            "2022-12-10 03:46:42.378807 Epoch 104, Training loss 0.6952079746043286\n",
            "2022-12-10 03:47:18.782372 Epoch 105, Training loss 0.6930145479148001\n",
            "2022-12-10 03:47:55.105006 Epoch 106, Training loss 0.6919682686362425\n",
            "2022-12-10 03:48:31.338910 Epoch 107, Training loss 0.6903434402268865\n",
            "2022-12-10 03:49:07.515123 Epoch 108, Training loss 0.6881123977091611\n",
            "2022-12-10 03:49:43.642784 Epoch 109, Training loss 0.6857414957888596\n",
            "2022-12-10 03:50:19.861020 Epoch 110, Training loss 0.685570102937691\n",
            "2022-12-10 03:50:56.338547 Epoch 111, Training loss 0.685069427389623\n",
            "2022-12-10 03:51:32.783266 Epoch 112, Training loss 0.681006771905343\n",
            "2022-12-10 03:52:10.338605 Epoch 113, Training loss 0.6807557957632767\n",
            "2022-12-10 03:52:45.334244 Epoch 114, Training loss 0.6798799254019242\n",
            "2022-12-10 03:53:22.937345 Epoch 115, Training loss 0.6771602798682039\n",
            "2022-12-10 03:53:57.915563 Epoch 116, Training loss 0.6775651967434018\n",
            "2022-12-10 03:54:35.190628 Epoch 117, Training loss 0.67489858440426\n",
            "2022-12-10 03:55:10.195855 Epoch 118, Training loss 0.6749849245523858\n",
            "2022-12-10 03:55:47.685281 Epoch 119, Training loss 0.6722220150024995\n",
            "2022-12-10 03:56:23.521946 Epoch 120, Training loss 0.6707205954567551\n",
            "2022-12-10 03:56:59.379350 Epoch 121, Training loss 0.6699319565311417\n",
            "2022-12-10 03:57:33.877743 Epoch 122, Training loss 0.6672780922306772\n",
            "2022-12-10 03:58:11.047022 Epoch 123, Training loss 0.6674183933707454\n",
            "2022-12-10 03:58:46.821604 Epoch 124, Training loss 0.6658949496038734\n",
            "2022-12-10 03:59:22.870988 Epoch 125, Training loss 0.664822091874869\n",
            "2022-12-10 03:59:57.491382 Epoch 126, Training loss 0.6633003157041871\n",
            "2022-12-10 04:00:34.964586 Epoch 127, Training loss 0.6616882255010288\n",
            "2022-12-10 04:01:11.220979 Epoch 128, Training loss 0.65953211036637\n",
            "2022-12-10 04:01:47.304636 Epoch 129, Training loss 0.6586358810172361\n",
            "2022-12-10 04:02:23.475325 Epoch 130, Training loss 0.65730192582778\n",
            "2022-12-10 04:02:59.707700 Epoch 131, Training loss 0.656773997504083\n",
            "2022-12-10 04:03:36.111265 Epoch 132, Training loss 0.6560771429858854\n",
            "2022-12-10 04:04:12.630912 Epoch 133, Training loss 0.6530017690051853\n",
            "2022-12-10 04:04:49.025977 Epoch 134, Training loss 0.6529308331515783\n",
            "2022-12-10 04:05:25.484309 Epoch 135, Training loss 0.6522529777282339\n",
            "2022-12-10 04:06:02.106190 Epoch 136, Training loss 0.6505161784875119\n",
            "2022-12-10 04:06:37.654777 Epoch 137, Training loss 0.6504183675703186\n",
            "2022-12-10 04:07:13.760622 Epoch 138, Training loss 0.6490225161966461\n",
            "2022-12-10 04:07:49.333711 Epoch 139, Training loss 0.6462728149064666\n",
            "2022-12-10 04:08:26.752849 Epoch 140, Training loss 0.6461200252213442\n",
            "2022-12-10 04:09:01.871663 Epoch 141, Training loss 0.6432673226841881\n",
            "2022-12-10 04:09:39.523098 Epoch 142, Training loss 0.6423182623160769\n",
            "2022-12-10 04:10:15.762143 Epoch 143, Training loss 0.6414524469991474\n",
            "2022-12-10 04:10:52.275622 Epoch 144, Training loss 0.6417251064649323\n",
            "2022-12-10 04:11:27.581901 Epoch 145, Training loss 0.6402329781171306\n",
            "2022-12-10 04:12:04.913674 Epoch 146, Training loss 0.6397229779483108\n",
            "2022-12-10 04:12:41.540194 Epoch 147, Training loss 0.6361334525868106\n",
            "2022-12-10 04:13:18.341006 Epoch 148, Training loss 0.637254903383572\n",
            "2022-12-10 04:13:55.034451 Epoch 149, Training loss 0.6369464113889143\n",
            "2022-12-10 04:14:31.757523 Epoch 150, Training loss 0.6348781878381129\n",
            "2022-12-10 04:15:09.082492 Epoch 151, Training loss 0.6341436577346319\n",
            "2022-12-10 04:15:44.676833 Epoch 152, Training loss 0.6339101984601496\n",
            "2022-12-10 04:16:22.514062 Epoch 153, Training loss 0.6303334109142156\n",
            "2022-12-10 04:16:57.872139 Epoch 154, Training loss 0.6309795274072901\n",
            "2022-12-10 04:17:35.451543 Epoch 155, Training loss 0.6295363999083828\n",
            "2022-12-10 04:18:10.440043 Epoch 156, Training loss 0.629264495485579\n",
            "2022-12-10 04:18:47.836081 Epoch 157, Training loss 0.6273618061143114\n",
            "2022-12-10 04:19:24.030430 Epoch 158, Training loss 0.6253263243781332\n",
            "2022-12-10 04:20:00.108287 Epoch 159, Training loss 0.6245769662091799\n",
            "2022-12-10 04:20:35.366677 Epoch 160, Training loss 0.6248745081750938\n",
            "2022-12-10 04:21:12.498355 Epoch 161, Training loss 0.6235461408067542\n",
            "2022-12-10 04:21:49.049893 Epoch 162, Training loss 0.6228598449236292\n",
            "2022-12-10 04:22:25.048799 Epoch 163, Training loss 0.6214374984088151\n",
            "2022-12-10 04:23:00.935766 Epoch 164, Training loss 0.6212955685642064\n",
            "2022-12-10 04:23:36.855736 Epoch 165, Training loss 0.61953614274864\n",
            "2022-12-10 04:24:13.930049 Epoch 166, Training loss 0.6188560872126723\n",
            "2022-12-10 04:24:48.550142 Epoch 167, Training loss 0.6178437921854542\n",
            "2022-12-10 04:25:26.118359 Epoch 168, Training loss 0.6181347119762465\n",
            "2022-12-10 04:26:02.492383 Epoch 169, Training loss 0.6161023964891044\n",
            "2022-12-10 04:26:38.955923 Epoch 170, Training loss 0.6157564845917475\n",
            "2022-12-10 04:27:14.214271 Epoch 171, Training loss 0.6141609906235619\n",
            "2022-12-10 04:27:52.069623 Epoch 172, Training loss 0.6137450030240257\n",
            "2022-12-10 04:28:28.418630 Epoch 173, Training loss 0.6112740442271123\n",
            "2022-12-10 04:29:04.555306 Epoch 174, Training loss 0.6104255217268034\n",
            "2022-12-10 04:29:39.540394 Epoch 175, Training loss 0.6118015470864523\n",
            "2022-12-10 04:30:17.063090 Epoch 176, Training loss 0.6098563260663196\n",
            "2022-12-10 04:30:54.638314 Epoch 177, Training loss 0.610037733641122\n",
            "2022-12-10 04:31:29.590558 Epoch 178, Training loss 0.6088552097301654\n",
            "2022-12-10 04:32:05.835562 Epoch 179, Training loss 0.6059843778152905\n",
            "2022-12-10 04:32:42.059288 Epoch 180, Training loss 0.6058148385961647\n",
            "2022-12-10 04:33:20.168214 Epoch 181, Training loss 0.6051320347106061\n",
            "2022-12-10 04:33:55.756738 Epoch 182, Training loss 0.6060834300258885\n",
            "2022-12-10 04:34:33.705993 Epoch 183, Training loss 0.6042982343288944\n",
            "2022-12-10 04:35:10.748975 Epoch 184, Training loss 0.6014460104963054\n",
            "2022-12-10 04:35:47.541816 Epoch 185, Training loss 0.6026543037360891\n",
            "2022-12-10 04:36:22.898202 Epoch 186, Training loss 0.6026274516911763\n",
            "2022-12-10 04:37:00.817113 Epoch 187, Training loss 0.6008187691726343\n",
            "2022-12-10 04:37:38.820286 Epoch 188, Training loss 0.6011769754807358\n",
            "2022-12-10 04:38:14.123618 Epoch 189, Training loss 0.5986679358707975\n",
            "2022-12-10 04:38:50.439123 Epoch 190, Training loss 0.5976536146667607\n",
            "2022-12-10 04:39:26.872223 Epoch 191, Training loss 0.5971208368344685\n",
            "2022-12-10 04:40:04.631965 Epoch 192, Training loss 0.5971032925274061\n",
            "2022-12-10 04:40:39.563042 Epoch 193, Training loss 0.5962016235303391\n",
            "2022-12-10 04:41:15.732450 Epoch 194, Training loss 0.5962519659029554\n",
            "2022-12-10 04:41:52.097410 Epoch 195, Training loss 0.595196900122306\n",
            "2022-12-10 04:42:29.233865 Epoch 196, Training loss 0.594385741921642\n",
            "2022-12-10 04:43:04.238651 Epoch 197, Training loss 0.5918565373820113\n",
            "2022-12-10 04:43:41.625054 Epoch 198, Training loss 0.5929923752689605\n",
            "2022-12-10 04:44:18.002806 Epoch 199, Training loss 0.5907790395228759\n",
            "2022-12-10 04:44:54.244723 Epoch 200, Training loss 0.5921871627078337\n",
            "2022-12-10 04:45:29.455715 Epoch 201, Training loss 0.5901161423119743\n",
            "2022-12-10 04:46:07.156374 Epoch 202, Training loss 0.5880124542643043\n",
            "2022-12-10 04:46:43.873387 Epoch 203, Training loss 0.5897052276622304\n",
            "2022-12-10 04:47:20.096166 Epoch 204, Training loss 0.5877607928212646\n",
            "2022-12-10 04:47:55.424824 Epoch 205, Training loss 0.5859802817292226\n",
            "2022-12-10 04:48:33.278215 Epoch 206, Training loss 0.5858836164483634\n",
            "2022-12-10 04:49:11.118367 Epoch 207, Training loss 0.585380706983759\n",
            "2022-12-10 04:49:46.109546 Epoch 208, Training loss 0.5851539685902998\n",
            "2022-12-10 04:50:22.446639 Epoch 209, Training loss 0.583496341619955\n",
            "2022-12-10 04:50:58.991728 Epoch 210, Training loss 0.5830977773651138\n",
            "2022-12-10 04:51:36.731917 Epoch 211, Training loss 0.5831690381097672\n",
            "2022-12-10 04:52:11.601708 Epoch 212, Training loss 0.5814789242832862\n",
            "2022-12-10 04:52:47.873277 Epoch 213, Training loss 0.5801223459298653\n",
            "2022-12-10 04:53:24.184287 Epoch 214, Training loss 0.5803722154606333\n",
            "2022-12-10 04:54:01.523689 Epoch 215, Training loss 0.5807453658803344\n",
            "2022-12-10 04:54:36.123576 Epoch 216, Training loss 0.5778478673466331\n",
            "2022-12-10 04:55:13.406866 Epoch 217, Training loss 0.5775520928451777\n",
            "2022-12-10 04:55:48.717003 Epoch 218, Training loss 0.5772913014492416\n",
            "2022-12-10 04:56:26.055137 Epoch 219, Training loss 0.5774645290106458\n",
            "2022-12-10 04:57:00.912513 Epoch 220, Training loss 0.5769123935790927\n",
            "2022-12-10 04:57:38.289760 Epoch 221, Training loss 0.575856597755876\n",
            "2022-12-10 04:58:14.618601 Epoch 222, Training loss 0.5764032366025783\n",
            "2022-12-10 04:58:50.605093 Epoch 223, Training loss 0.5750644473011232\n",
            "2022-12-10 04:59:25.464235 Epoch 224, Training loss 0.5738661628016426\n",
            "2022-12-10 05:00:02.837707 Epoch 225, Training loss 0.5739620186941093\n",
            "2022-12-10 05:00:39.335195 Epoch 226, Training loss 0.572228755449395\n",
            "2022-12-10 05:01:15.046926 Epoch 227, Training loss 0.5721733337244415\n",
            "2022-12-10 05:01:51.240838 Epoch 228, Training loss 0.5730151866784181\n",
            "2022-12-10 05:02:27.451150 Epoch 229, Training loss 0.5711650285498261\n",
            "2022-12-10 05:03:05.083737 Epoch 230, Training loss 0.5697119587370197\n",
            "2022-12-10 05:03:40.005341 Epoch 231, Training loss 0.5699480626246204\n",
            "2022-12-10 05:04:16.208698 Epoch 232, Training loss 0.5691477022969814\n",
            "2022-12-10 05:04:51.882580 Epoch 233, Training loss 0.5672681684536702\n",
            "2022-12-10 05:05:29.235896 Epoch 234, Training loss 0.568622444520521\n",
            "2022-12-10 05:06:04.356386 Epoch 235, Training loss 0.5668075029807322\n",
            "2022-12-10 05:06:41.972831 Epoch 236, Training loss 0.5663623638889369\n",
            "2022-12-10 05:07:17.070931 Epoch 237, Training loss 0.5665108598101779\n",
            "2022-12-10 05:07:54.654359 Epoch 238, Training loss 0.5657906670628301\n",
            "2022-12-10 05:08:29.759387 Epoch 239, Training loss 0.5662314795395907\n",
            "2022-12-10 05:09:07.209910 Epoch 240, Training loss 0.5641472676144842\n",
            "2022-12-10 05:09:42.273511 Epoch 241, Training loss 0.5634196497633329\n",
            "2022-12-10 05:10:20.266702 Epoch 242, Training loss 0.563310876926955\n",
            "2022-12-10 05:10:55.887255 Epoch 243, Training loss 0.56472687697624\n",
            "2022-12-10 05:11:34.117347 Epoch 244, Training loss 0.5632404547251398\n",
            "2022-12-10 05:12:09.544486 Epoch 245, Training loss 0.5627024755301073\n",
            "2022-12-10 05:12:47.578252 Epoch 246, Training loss 0.561063945445868\n",
            "2022-12-10 05:13:23.795889 Epoch 247, Training loss 0.5609985397142523\n",
            "2022-12-10 05:14:01.271404 Epoch 248, Training loss 0.560353948255939\n",
            "2022-12-10 05:14:36.879321 Epoch 249, Training loss 0.5593620010501589\n",
            "2022-12-10 05:15:12.799698 Epoch 250, Training loss 0.5592030175506612\n",
            "2022-12-10 05:15:52.613652 Epoch 251, Training loss 0.5576003653847653\n",
            "2022-12-10 05:16:29.630536 Epoch 252, Training loss 0.5578449149723248\n",
            "2022-12-10 05:17:05.181638 Epoch 253, Training loss 0.5548973098549697\n",
            "2022-12-10 05:17:40.691584 Epoch 254, Training loss 0.5551940251494307\n",
            "2022-12-10 05:18:16.081386 Epoch 255, Training loss 0.555702110195099\n",
            "2022-12-10 05:18:54.247997 Epoch 256, Training loss 0.5565075706261808\n",
            "2022-12-10 05:19:33.291703 Epoch 257, Training loss 0.5549963107118216\n",
            "2022-12-10 05:20:09.115846 Epoch 258, Training loss 0.5529825000278176\n",
            "2022-12-10 05:20:45.436372 Epoch 259, Training loss 0.5558848232793077\n",
            "2022-12-10 05:21:23.469780 Epoch 260, Training loss 0.5528804558088712\n",
            "2022-12-10 05:21:59.339143 Epoch 261, Training loss 0.5527724937900252\n",
            "2022-12-10 05:22:38.275171 Epoch 262, Training loss 0.5519241503132578\n",
            "2022-12-10 05:23:17.021941 Epoch 263, Training loss 0.5531079788189714\n",
            "2022-12-10 05:23:52.297414 Epoch 264, Training loss 0.5518527803823466\n",
            "2022-12-10 05:24:27.608764 Epoch 265, Training loss 0.549166991201508\n",
            "2022-12-10 05:25:03.049186 Epoch 266, Training loss 0.551693646689815\n",
            "2022-12-10 05:25:38.373683 Epoch 267, Training loss 0.5501275916233697\n",
            "2022-12-10 05:26:19.702144 Epoch 268, Training loss 0.5492040514183776\n",
            "2022-12-10 05:26:55.495662 Epoch 269, Training loss 0.5485665793423458\n",
            "2022-12-10 05:27:31.306846 Epoch 270, Training loss 0.5470128533289865\n",
            "2022-12-10 05:28:07.114064 Epoch 271, Training loss 0.5492001327559771\n",
            "2022-12-10 05:28:45.493795 Epoch 272, Training loss 0.5465680008272991\n",
            "2022-12-10 05:29:23.824740 Epoch 273, Training loss 0.5462256031268088\n",
            "2022-12-10 05:29:58.964298 Epoch 274, Training loss 0.5462068876113428\n",
            "2022-12-10 05:30:34.012546 Epoch 275, Training loss 0.5459199492507578\n",
            "2022-12-10 05:31:09.204669 Epoch 276, Training loss 0.5457878918827647\n",
            "2022-12-10 05:31:47.535698 Epoch 277, Training loss 0.5441821609311701\n",
            "2022-12-10 05:32:23.361788 Epoch 278, Training loss 0.5438630965435901\n",
            "2022-12-10 05:33:01.602051 Epoch 279, Training loss 0.5441621033012715\n",
            "2022-12-10 05:33:36.980758 Epoch 280, Training loss 0.5434566446391823\n",
            "2022-12-10 05:34:12.381059 Epoch 281, Training loss 0.5416014748613548\n",
            "2022-12-10 05:34:50.991501 Epoch 282, Training loss 0.541983375159066\n",
            "2022-12-10 05:35:26.823052 Epoch 283, Training loss 0.5417525323722369\n",
            "2022-12-10 05:36:05.470075 Epoch 284, Training loss 0.5417101353483127\n",
            "2022-12-10 05:36:40.933641 Epoch 285, Training loss 0.5409825972431456\n",
            "2022-12-10 05:37:16.398205 Epoch 286, Training loss 0.5415699308942956\n",
            "2022-12-10 05:37:52.079497 Epoch 287, Training loss 0.5387486620517947\n",
            "2022-12-10 05:38:31.193981 Epoch 288, Training loss 0.5402641098022156\n",
            "2022-12-10 05:39:08.691238 Epoch 289, Training loss 0.5412719723056344\n",
            "2022-12-10 05:39:46.417117 Epoch 290, Training loss 0.5394862364129642\n",
            "2022-12-10 05:40:22.728976 Epoch 291, Training loss 0.5393007057707023\n",
            "2022-12-10 05:40:58.474581 Epoch 292, Training loss 0.5382206033715202\n",
            "2022-12-10 05:41:36.909750 Epoch 293, Training loss 0.5367006249249439\n",
            "2022-12-10 05:42:12.862460 Epoch 294, Training loss 0.5365980101363433\n",
            "2022-12-10 05:42:51.603937 Epoch 295, Training loss 0.5355404314330167\n",
            "2022-12-10 05:43:27.482226 Epoch 296, Training loss 0.536285498753533\n",
            "2022-12-10 05:44:03.449358 Epoch 297, Training loss 0.5349653232509218\n",
            "2022-12-10 05:44:42.754709 Epoch 298, Training loss 0.5339936440253197\n",
            "2022-12-10 05:45:18.875600 Epoch 299, Training loss 0.533825367917795\n",
            "2022-12-10 05:45:57.827408 Epoch 300, Training loss 0.5338574371984243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy for 2a - CNN - 2 layers\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "CjxX_6UPXIfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "841b0163-9d92-4f97-c305-7c03bf4f432f"
      },
      "id": "CjxX_6UPXIfo",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.82\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}